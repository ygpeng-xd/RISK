import pandas as pd
import time
import sys
import os

import InvertedIndex
import depth_test
import query_test
import correct_testPlus_knn
import AES_sha1

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from quad_index import quad_token


def analyze_query_coverage(query_words, query_rect, matched_data, invert_file_path):
    # 1. ä»å€’æ’ç´¢å¼•æ–‡ä»¶è¯»å–åŸå§‹æ•°æ®
    original_points = []
    with open(invert_file_path, 'r') as f:
        for line in f:
            parts = line.strip().split(' ')
            keyword = parts[0]
            if keyword == query_words:
                # æå–æ‰€æœ‰åæ ‡å­—ç¬¦ä¸²ï¼Œå»æ‰æ‹¬å·
                coord_strs = [part.strip('()') for part in parts[1:]]
                # ä½¿ç”¨æ­¥é•¿ä¸º2çš„åˆ‡ç‰‡åˆ†åˆ«è·å–xå’Œyåæ ‡
                original_points = [(int(coord_strs[i]), int(coord_strs[i + 1]))
                                   for i in range(0, len(coord_strs), 2)]
                break
    # print(f"å…³é”®è¯ '{query_words}' åœ¨åŸå§‹æ•°æ®é›†ä¸­å…±æœ‰ {len(original_points)} ä¸ªç‚¹")

    # 2. è®¡ç®—åœ¨æŸ¥è¯¢èŒƒå›´å†…çš„åŸå§‹ç‚¹
    x_min, x_max, y_min, y_max = query_rect
    points_in_range = []
    for x, y in original_points:
        if x_min <= x <= x_max and y_min <= y <= y_max:
            points_in_range.append((x, y))
    # print(f"åœ¨æŸ¥è¯¢èŒƒå›´å†…çš„åŸå§‹ç‚¹æ•°é‡: {len(points_in_range)}")

    # 3. è§£ææŸ¥è¯¢ç»“æœä¸­çš„ç‚¹
    returned_points = [(int(x), int(y))
                       for data_line in matched_data
                       for part in data_line.split()[1:]
                       if ',' in part
                       for x, y in [part.strip('()').split(',')]]

    # 4. è®¡ç®—äº¤é›† - å®é™…è¿”å›çš„æ­£ç¡®ç‚¹
    points_in_range_set = set(points_in_range)
    returned_points_set = set(returned_points)
    # print(f"æŸ¥è¯¢ç»“æœè¿”å›çš„ç‚¹æ•°é‡: {len(returned_points_set)}")

    correct_returned = points_in_range_set.intersection(returned_points_set)
    # print(f"æŸ¥è¯¢ç»“æœä¸­æ­£ç¡®çš„ç‚¹æ•°é‡: {len(correct_returned)}")

    # 5. è®¡ç®—å¬å›ç‡å’Œç²¾ç¡®ç‡
    recall = len(correct_returned) / len(points_in_range_set) if len(points_in_range_set) > 0 else 0
    precision = len(correct_returned) / len(returned_points_set) if len(returned_points_set) > 0 else 0

    # 6. æ˜¾ç¤ºè¯¦ç»†ç»Ÿè®¡
    return len(points_in_range_set), recall, precision


def analyze_query_coverage_plus(query_rect, matched_data1, matched_data2):
    x_min, x_max, y_min, y_max = query_rect

    # 1. ä» matched_data1 ä¸­è§£æç‚¹
    returned_points1 = [(int(x), int(y))
                        for data_line in matched_data1
                        for part in data_line.split()[1:]
                        if ',' in part
                        for x, y in [part.strip('()').split(',')]]

    # 2. åˆ¤æ–­å“ªäº›ç‚¹åœ¨ query_rect ä¸­ï¼Œä½œä¸ºç²¾ç¡®è§£
    exact_points1 = [(x, y) for x, y in returned_points1
                     if x_min <= x <= x_max and y_min <= y <= y_max]

    returned_points_set1 = set(returned_points1)
    points_in_range_set1 = set(exact_points1)

    # 3. ä» matched_data2 ä¸­è§£æç‚¹
    returned_points2 = [(int(x), int(y))
                        for data_line in matched_data2
                        for part in data_line.split()[1:]
                        if ',' in part
                        for x, y in [part.strip('()').split(',')]]

    # 4. åˆ¤æ–­å“ªäº›ç‚¹åœ¨ query_rect ä¸­ï¼Œä½œä¸ºç²¾ç¡®è§£
    exact_points2 = [(x, y) for x, y in returned_points2
                     if x_min <= x <= x_max and y_min <= y <= y_max]

    returned_points_set2 = set(returned_points2)
    points_in_range_set2 = set(exact_points2)

    # 5. è®¡ç®—åŒå…³é”®è¯çš„ç»“æœ
    # è¿”å›ç‚¹ = ä¸¤ä¸ªé›†åˆçš„å¹¶é›†
    returned_points_set = returned_points_set1.union(returned_points_set2)

    # ç²¾ç¡®è§£ = ä¸¤ä¸ªç²¾ç¡®è§£é›†åˆçš„äº¤é›†
    points_in_range_set = points_in_range_set1.intersection(points_in_range_set2)

    # 6. recall å›ºå®šä¸º 1
    recall = 1 if len(returned_points_set) > 0 else 0

    # 7. precision = ç²¾ç¡®è§£ä¸ªæ•° / è¿”å›ç‚¹ä¸ªæ•°
    if len(points_in_range_set) == 0:
        precision = 1.0
    else:
        precision = len(points_in_range_set) / len(returned_points_set) if returned_points_set else 0

    return len(points_in_range_set), recall, precision


def format_scientific(value, decimal_places=4):
    """
    å°†æ•°å€¼æ ¼å¼åŒ–ä¸ºç§‘å­¦è®¡æ•°æ³•ï¼Œä¿ç•™æŒ‡å®šå°æ•°ä½æ•°
    ä¾‹å¦‚ï¼š0.00002557 -> "2.5570e-05"
    è¿”å›floatç±»å‹ è¿™æ ·Excelèƒ½æ­£ç¡®è¯†åˆ«ä¸ºæ•°å€¼è€Œä¸æ˜¯æ–‡æœ¬
    """
    if value == 0:
        return "0.0000e+00"
    return float(f"{value:.{decimal_places}e}")


def update_csv_with_results(csv_filepath, K, dataset_name, algorithm_type, result, group):
    """
    æ›´æ–°CSVæ–‡ä»¶ä¸­å¯¹åº”Kå€¼è¡Œçš„ç»Ÿè®¡ç»“æœ
    """
    # è¯»å–ç°æœ‰çš„CSVæ–‡ä»¶ï¼Œå¹¶æŒ‡å®šå­—ç¬¦ä¸²åˆ—çš„æ•°æ®ç±»å‹
    try:
        # æŒ‡å®šéœ€è¦å­˜å‚¨å­—ç¬¦ä¸²çš„åˆ—ä¸ºobjectç±»å‹
        dtype_dict = {
            'database': 'object',
        }
        df = pd.read_csv(csv_filepath, dtype=dtype_dict)
    except FileNotFoundError:
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_filepath}")
        return

    # æ‰¾åˆ°å¯¹åº”Kå€¼çš„è¡Œ
    k_row_index = df[
        (df['K'] == K) &
        (df['group'] == group) &
        (df['range_para'] == result['range_para'])
        ].index

    if len(k_row_index) == 0:
        # åˆ›å»ºæ–°è¡Œæ•°æ®
        print(f"ğŸ†• åˆ›å»ºæ–°è¡Œ: K={K}, group={group}, range_para={result['range_para']}")
        # è·å–æ–°è¡Œçš„ç´¢å¼•ï¼ˆä¸‹ä¸€ä¸ªå¯ç”¨çš„ç´¢å¼•ï¼‰
        k_row_index = len(df)
        # ä½¿ç”¨locç›´æ¥åˆ›å»ºæ–°è¡Œï¼Œé¿å…concatè­¦å‘Š
        df.loc[k_row_index, 'K'] = K
        df.loc[k_row_index, 'group'] = group
        df.loc[k_row_index, 'range_para'] = result['range_para']
        # åˆå§‹åŒ–å…¶ä»–åˆ—ä¸ºé€‚å½“é»˜è®¤å€¼
        for col in df.columns:
            if col not in ['K', 'group', 'range_para']:
                df.loc[k_row_index, col] = None
    else:
        # å¦‚æœæ‰¾åˆ°äº†å¯¹åº”çš„è¡Œï¼Œè·å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„è¡Œç´¢å¼•
        k_row_index = k_row_index[0]
        print(f"ğŸ”„ æ›´æ–°ç°æœ‰è¡Œ: K={K}, group={group}, range_para={result['range_para']}")

    # å¡«å†™åŸºæœ¬ä¿¡æ¯
    df.loc[k_row_index, 'database'] = dataset_name
    df.loc[k_row_index, 'range_query'] = 1  # èŒƒå›´æŸ¥è¯¢å›ºå®šä¸º1
    df.loc[k_row_index, 'query_number'] = result['total_tests']
    df.loc[k_row_index, 'tree_min_depth'] = result['min_depth']
    df.loc[k_row_index, 'tree_max_depth'] = result['max_depth']

    # èŒƒå›´æŸ¥è¯¢
    if algorithm_type == 'center':
        df.loc[k_row_index, 'use_center_file'] = 1
    elif algorithm_type == 'centroid':
        df.loc[k_row_index, 'use_centroid_file'] = 1

    # å¡«å†™æ€§èƒ½ç»Ÿè®¡æ•°æ®ï¼ˆæ—¶é—´æ•°æ®ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•æ ¼å¼ï¼‰
    df.loc[k_row_index, 'token_time_avg(s)'] = format_scientific(result['avg_token_time'])
    df.loc[k_row_index, 'hash_token_time_avg(s)'] = format_scientific(result['avg_hash_token_time'])
    df.loc[k_row_index, 'matched_time_avg(s)'] = format_scientific(result['avg_matched_time'])
    df.loc[k_row_index, 'decrypt_time_avg(s)'] = format_scientific(result['avg_decrypt_time'])
    df.loc[k_row_index, 'refine_time_avg(s)'] = format_scientific(result['avg_refine_time'])

    # å…¶ä»–æ•°æ®ä¿æŒåŸæœ‰æ ¼å¼
    df.loc[k_row_index, 'results_count_avg'] = result['avg_results_count']
    df.loc[k_row_index, 'token_count_avg'] = result['avg_token_count']
    df.loc[k_row_index, 'candidate_count_avg'] = result['avg_candidate_count']
    df.loc[k_row_index, 'token_size_avg(bytes)'] = result['avg_token_size']
    df.loc[k_row_index, 'candidate_size_avg(bytes)'] = result['avg_candidate_size']
    df.loc[k_row_index, 'recall_avg'] = round(result['avg_recall'], 4)
    df.loc[k_row_index, 'precision_avg'] = round(result['avg_precision'], 4)
    # èŒƒå›´æŸ¥è¯¢æŸ¥è¯¢ä¸å¡«å†™rationï¼ˆä¿æŒä¸ºç©ºï¼‰

    # å¡«å†™æœ€åä¿®æ”¹æ—¶é—´
    df.loc[k_row_index, 'last_modify_time'] = int(time.strftime("%d%H%M%S", time.localtime()))

    # ä¿å­˜æ›´æ–°åçš„CSVæ–‡ä»¶
    df.to_csv(csv_filepath, index=False, encoding='utf-8', sep=',')
    print(f"âœ… å·²æ›´æ–°CSVæ–‡ä»¶ä¸­K={K} group={group} range_para={result['range_para']}çš„ç»Ÿè®¡ç»“æœ")


if __name__ == '__main__':
    original_filepath = None
    test_filepath = None
    csv_filepath = '../../exp_result/data_statistics0828.csv'
    base_dir = '../../Cdatabase'

    """
    é…ç½®åŒºåŸŸ - è‡ªåŠ¨æµ‹è¯•å¤šä¸ªKå€¼
    """
    K = 80  # è¦æµ‹è¯•çš„Kå€¼åˆ—è¡¨
    datasets_name = ['twitter', 'newyork', 'paris']
    algorithm_type = 'center'  # 'center' æˆ–è€… 'centroid'
    # èŒƒå›´æŸ¥è¯¢ è¯¥ç»“æœå§‹ç»ˆä¸ºfalse
    not_voronoi_query = False
    exp_group = 4

    x_min, x_max, y_min, y_max = 0, 100000000, 0, 100000000
    initial_bounds = [x_min, x_max, y_min, y_max]

    # ========== å¾ªç¯æ­£å¼å¼€å§‹ ==========
    # ========== 20250823 è¿™é‡Œæ˜¯ä»¥æ•°æ®é›†æ§åˆ¶å¾ªç¯ ä¸‹é¢ä»¥èŒƒå›´æ§åˆ¶å¾ªç¯ ==========
    for dataset_name in datasets_name:
        EXPERIMENT_GROUP = exp_group
        exp_group += 1
        if dataset_name == 'twitter':
            original_filepath = '../../original_dataset/twitter_new_RASK108.txt'
            test_filepath = '../../Adatabase/twitter_test_2.txt'
        elif dataset_name == 'newyork':
            original_filepath = '../../original_dataset/newyork_RASK108.txt'
            test_filepath = '../../Adatabase/newyork_test_2.txt'
        elif dataset_name == 'paris':
            original_filepath = '../../original_dataset/Paris_RASK108.txt'
            test_filepath = '../../Adatabase/paris_test_2.txt'
        inverted_index = InvertedIndex.build_inverted_index_v2(original_filepath)
        test_keywords_list, test_points = correct_testPlus_knn.load_test_data(test_filepath)

        # ========== æ±‡æ€»ç»“æœç»Ÿè®¡ ==========
        total_results = {}  # å­˜å‚¨æ¯ä¸ªèŒƒå›´çš„æµ‹è¯•ç»“æœ
        # ========== å¾ªç¯æµ‹è¯•1%åˆ°5%çš„åŠå¾„ ==========
        setR = 5000000000000000  # 5e15
        percent_first = 0.01
        percent_second = 0.05
        int_percent_first = int(percent_first * 100)  # 1
        int_percent_second = int(percent_second * 100)  # 5
        now_percent_first = percent_first
        for percent_int in range(int_percent_first, int_percent_second + 1):  # 1~5
            # å®éªŒç»„é…ç½®
            percent = now_percent_first
            now_percent_first += 0.01
            print(f"å½“å‰æ•°æ®é›†ï¼š{dataset_name} å½“å‰åŠå¾„æ§åˆ¶å‚æ•°: {percent_int}% çš„åŠå¾„æŸ¥è¯¢ (å®éªŒç»„: {EXPERIMENT_GROUP})")

            # åˆ›é€ æŸ¥è¯¢åŠå¾„
            radius_ratio = percent * percent
            r2 = setR * radius_ratio
            radius = int(r2 ** 0.5)

            print("=" * 80)
            print(f"ğŸ” å¼€å§‹æµ‹è¯• K={K:2d}")
            print("=" * 80)

            # ========== å…¨éƒ¨ç”¨K=è®¾å®šå€¼çš„æ–‡ä»¶ ==========
            generate_filepath = f'{base_dir}/{dataset_name}_quadtree_{algorithm_type}{K}.txt'
            hash_filepath = f'{base_dir}/{dataset_name}_quadtree_{algorithm_type}{K}_hash.txt'

            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(generate_filepath) or not os.path.exists(hash_filepath):
                print(f"âŒ K={K:2d} çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•")
                total_results[percent_int] = "æ–‡ä»¶ä¸å­˜åœ¨"
                continue

            # ========== è‡ªåŠ¨è®¡ç®—æœ€å¤§æœ€å°æ·±åº¦ ==========
            max_depth, min_depth = [x // 2 for x in depth_test.count_max_min_length(generate_filepath)]

            hash_dict = {}
            with open(hash_filepath, 'r') as hash_file:
                for line in hash_file:
                    line = line.strip()
                    if line:  # è·³è¿‡ç©ºè¡Œ
                        parts = line.split(' ', 1)
                        if len(parts) >= 2:
                            hash_key = parts[0]
                            hash_dict[hash_key] = line  # ä¿å­˜å®Œæ•´è¡Œ

            # è¿›è¡Œæ‰¹é‡æŸ¥è¯¢
            test_count = 0
            # ========== è¯¥Kå€¼æµ‹è¯•çš„å„ç§æ—¶é—´ç»Ÿè®¡ ==========
            k_time = []
            k_token_time = []         # ç»Ÿè®¡æ˜æ–‡tokenç”Ÿæˆæ—¶é—´
            k_hash_token_time = []    # ç»Ÿè®¡hash_tokenç”Ÿæˆæ—¶é—´
            k_matched_time = []       # ç»Ÿè®¡åŒ¹é…æ—¶é—´
            k_refine_time = []        # ç»Ÿè®¡ç²¾ç‚¼ç»“æœæ—¶é—´
            k_decrypt_time = []       # ç»Ÿè®¡å¯¹ç§°è§£å¯†æ—¶é—´
            # ========== ç»Ÿè®¡è¯¥Kå€¼æµ‹è¯•çš„å€™é€‰ç‚¹æ•°é‡ ==========
            result_counts = []        # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢ç²¾ç¡®ç»“æœçš„å¹³å‡ç‚¹æ•°é‡
            token_counts = []         # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢tokenæ•°é‡
            candidate_counts = []     # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢å€™é€‰é›†çš„å¹³å‡ç‚¹æ•°é‡
            token_sizes = []          # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢tokenå¤§å°--å­—èŠ‚æ•°
            candidate_sizes = []      # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢å€™é€‰é›†å¤§å°--å­—èŠ‚æ•°
            # ========== å¬å›ç‡å’Œç²¾ç¡®ç‡ ==========
            recall_list = []          # ç»Ÿè®¡å½“å‰Kå€¼æŸ¥è¯¢å¬å›ç‡
            precision_list = []       # ç»Ÿè®¡å½“å‰Kå€¼æŸ¥è¯¢ç²¾ç¡®ç‡

            for keywords, point in zip(test_keywords_list, test_points):
                test_count += 1
                keyword1 = keywords[0]
                keyword2 = keywords[1]
                # åˆ©ç”¨pointå¾—åˆ°query_rect
                query_rect = (point[0] - radius, point[0] + radius, point[1] - radius, point[1] + radius)

                print(f"[{test_count}/{len(test_keywords_list)}]--æŸ¥è¯¢èŒƒå›´: {query_rect}")

                # åˆ†æ‰¹å¤„ç†å¹¶ç›´æ¥åŒ¹é…
                print(f"  å…³é”®è¯1: {keyword1}")
                matched_data1 = []
                hash_token_list1 = []
                matched_time1 = 0
                hash_token_time1 = 0

                print(f"  å…³é”®è¯2: {keyword2}")
                matched_data2 = []
                hash_token_list2 = []
                matched_time2 = 0
                hash_token_time2 = 0

                batch_count = 0
                # ========== å¼€å§‹è®¡æ—¶ ==========
                start_time1 = time.perf_counter()
                for batch_result in quad_token.query_quadtree_range_optimized_batch(min_depth, max_depth, x_min, x_max, y_min, y_max, query_rect, batch_size=10000):
                    batch_count += 1

                    # æ„å»ºå½“å‰æ‰¹æ¬¡çš„result_list
                    if batch_count == 1:
                        # åªæœ‰ç¬¬ä¸€æ‰¹éœ€è¦"-1"
                        token_list = ["-1"]
                    else:
                        # åç»­æ‰¹æ¬¡ä¸éœ€è¦"-1"
                        token_list = []

                    for depth in sorted(batch_result.keys()):
                        token_list += batch_result[depth]

                    batch_start_time1 = time.perf_counter()
                    # ç›´æ¥å¯¹å½“å‰æ‰¹æ¬¡è¿›è¡ŒåŒ¹é…
                    batch_matched_data1, batch_hash_token_time1, batch_hash_token_list1 = query_test.match_query_results(keyword1, token_list, hash_dict, initial_bounds, min_depth, max_depth, not_voronoi_query)
                    batch_matched_time_with_hash_token1 = time.perf_counter() - batch_start_time1
                    batch_matched_time1 = batch_matched_time_with_hash_token1 - batch_hash_token_time1

                    batch_start_time1e = time.perf_counter()
                    # ç›´æ¥å¯¹å½“å‰æ‰¹æ¬¡è¿›è¡ŒåŒ¹é…
                    batch_matched_data2, batch_hash_token_time2, batch_hash_token_list2 = query_test.match_query_results(keyword2, token_list, hash_dict, initial_bounds, min_depth, max_depth, not_voronoi_query)
                    batch_matched_time_with_hash_token2 = time.perf_counter() - batch_start_time1e
                    batch_matched_time2 = batch_matched_time_with_hash_token2 - batch_hash_token_time2

                    # ç´¯ç§¯ç»“æœ
                    matched_data1.extend(batch_matched_data1)
                    hash_token_list1.extend(batch_hash_token_list1)
                    matched_time1 += batch_matched_time1
                    hash_token_time1 += batch_hash_token_time1

                    matched_data2.extend(batch_matched_data2)
                    hash_token_list2.extend(batch_hash_token_list2)
                    matched_time2 += batch_matched_time2
                    hash_token_time2 += batch_hash_token_time2

                matched_data = matched_data1 + matched_data2
                hash_token_list = hash_token_list1 + hash_token_list2
                matched_time = matched_time1 + matched_time2
                hash_token_time = hash_token_time1 + hash_token_time2

                # ç»Ÿè®¡æ˜æ–‡tokenç”Ÿæˆæ—¶é—´ï¼ˆåŒ…å«åŒ¹é…æ—¶é—´ï¼Œä½†è¿™æ ·æ›´åˆç†ï¼‰
                all_batch_total_time = time.perf_counter() - start_time1
                # tokenç”Ÿæˆæ—¶é—´ = æ€»æ—¶é—´ - åŒ¹é…æ—¶é—´ - hashåŠ å¯†æ—¶é—´
                token_time = all_batch_total_time - matched_time - hash_token_time

                result_set = set()
                for data in matched_data:
                    parts = data.split()
                    for part in parts[1:]:  # ä»ç¬¬äºŒä¸ªå…ƒç´ å¼€å§‹æ˜¯åæ ‡ç‚¹
                        result_set.add(part)
                result_tuples = [eval(item) for item in list(result_set)]

                # æŸ¥è¯¢æ—¶é•¿
                end_time = time.perf_counter()

                """
                ä¸‹é¢çš„è¿™ä¸€æ®µæ¨¡æ‹Ÿæš´æœç²¾ç‚¼ç»“æœ, ç»Ÿè®¡ç²¾ç‚¼æ—¶é—´
                """
                start_time3 = time.perf_counter()
                result_len, recall, precision = analyze_query_coverage_plus(query_rect, matched_data1, matched_data2)
                refine_time = time.perf_counter() - start_time3

                """
                ä¸‹é¢çš„è¿™ä¸€æ®µæ¨¡å †æˆåŠ è§£å¯†è¿‡ç¨‹ï¼Œå®é™…ä¸Šå°±ç»Ÿè®¡ä¸ªè§£å¯†çš„æ—¶é•¿è€Œå·²
                """
                # ========== æ–°å¢ï¼šå¯¹ç§°è§£å¯†æ—¶é—´ç»Ÿè®¡ ==========
                decrypt_time = 0
                if AES_sha1 is not None and matched_data:
                    # è®¾ç½®å¯¹ç§°åŠ å¯†å¯†é’¥
                    symmetric_key = 'xducc02241931xdu'  # ä½¿ç”¨ä¸åŸä»£ç ç›¸åŒçš„å¯†é’¥

                    # é¦–å…ˆæ¨¡æ‹ŸåŠ å¯†è¿‡ç¨‹ï¼šå°†æ¯æ¡æ•°æ®çš„ç‚¹åæ ‡éƒ¨åˆ†è¿›è¡Œå¯¹ç§°åŠ å¯†
                    encrypted_data_list = []
                    for data in matched_data:
                        parts = data.split(' ', 1)  # åˆ†å‰²hashç´¢å¼•å’Œç‚¹åæ ‡éƒ¨åˆ†
                        if len(parts) == 2:
                            hash_index = parts[0]
                            points_str = parts[1]  # æ‰€æœ‰ç‚¹åæ ‡çš„å­—ç¬¦ä¸²
                            # å¯¹ç‚¹åæ ‡éƒ¨åˆ†è¿›è¡ŒAESåŠ å¯†
                            encrypted_points = AES_sha1.AES_Encrypt(symmetric_key, points_str)
                            encrypted_data_list.append((hash_index, encrypted_points))

                    # å¼€å§‹è®¡æ—¶ï¼šå¯¹ç§°è§£å¯†è¿‡ç¨‹
                    start_time4 = time.perf_counter()
                    # å¯¹æ¯æ¡åŠ å¯†æ•°æ®è¿›è¡Œè§£å¯†
                    decrypted_data_list = []
                    for hash_index, encrypted_points in encrypted_data_list:
                        # å¯¹å¯†æ–‡è¿›è¡ŒAESè§£å¯†
                        decrypted_points = AES_sha1.AES_Decrypt(symmetric_key, encrypted_points)
                        decrypted_data_list.append((hash_index, decrypted_points))
                    # ç»“æŸè®¡æ—¶
                    decrypt_time = time.perf_counter() - start_time4

                # ========== ç»Ÿè®¡å„ç§å®éªŒç»“æœ ==========
                k_time.append(end_time - start_time1)
                k_token_time.append(token_time)
                k_hash_token_time.append(hash_token_time)
                k_matched_time.append(matched_time)
                k_refine_time.append(refine_time)
                k_decrypt_time.append(decrypt_time)
                result_counts.append(result_len)
                token_counts.append(len(hash_token_list))
                candidate_counts.append(len(result_tuples))
                token_sizes.append(sum(len(token.encode('utf-8')) for token in hash_token_list))
                candidate_sizes.append(sum(len(repr(tup).encode('utf-8')) for tup in result_tuples))
                recall_list.append(recall)
                precision_list.append(precision)

                print(f"  â±ï¸ æŸ¥è¯¢æ€»è€—æ—¶: {end_time - start_time1:.6f}ç§’")
                print(f"  â±ï¸ æ˜æ–‡tokenç”Ÿæˆè€—æ—¶: {token_time:.6f}ç§’")
                print(f"  â±ï¸ hashåŠ å¯†tokenç”Ÿæˆè€—æ—¶: {hash_token_time:.6f}ç§’")
                print(f"  â±ï¸ åŒ¹é…è€—æ—¶: {matched_time:.6f}ç§’")
                print(f"  â±ï¸ å¯¹ç§°è§£å¯†è€—æ—¶: {decrypt_time:.6f}ç§’")
                print(f"  â±ï¸ ç²¾ç‚¼è€—æ—¶: {refine_time:.6f}ç§’")
                print(f"  â±ï¸ ç²¾ç¡®ç»“æœæ•°é‡: {result_len}")
                print(f"  â±ï¸ tokenæ•°é‡: {len(hash_token_list)}")
                print(f"  â±ï¸ å€™é€‰ç‚¹æ•°é‡: {len(result_tuples)}")
                print(f"  â±ï¸ tokenå¤§å°: {sum(len(token.encode('utf-8')) for token in hash_token_list)}")
                print(f"  â±ï¸ å€™é€‰ç‚¹å¤§å°: {sum(len(repr(tup).encode('utf-8')) for tup in result_tuples)}")
                print(f"  â±ï¸ å¬å›ç‡: {recall:.6f}")
                print(f"  â±ï¸ ç²¾ç¡®ç‡: {precision:.6f}")

            # ========== è¯¥èŒƒå›´å€¼æµ‹è¯•ç»“æŸè®¡æ—¶ ==========
            k_total_time = sum(k_time)
            # è®°å½•è¯¥Kå€¼çš„æµ‹è¯•ç»“æœ
            total_results[percent_int] = {
                'min_depth': min_depth,
                'max_depth': max_depth,
                'total_tests': test_count,
                'test_time': k_total_time,
                'range_para': percent,
                'avg_token_time': sum(k_token_time) / test_count,
                'avg_hash_token_time': sum(k_hash_token_time) / test_count,
                'avg_matched_time': sum(k_matched_time) / test_count,
                'avg_refine_time': sum(k_refine_time) / test_count,
                'avg_decrypt_time': sum(k_decrypt_time) / test_count,
                'avg_results_count': sum(result_counts) / test_count,
                'avg_token_count': sum(token_counts) / test_count,
                'avg_candidate_count': sum(candidate_counts) / test_count,
                'avg_token_size': sum(token_sizes) / test_count,
                'avg_candidate_size': sum(candidate_sizes) / test_count,
                'avg_recall': sum(recall_list) / test_count,
                'avg_precision': sum(precision_list) / test_count
            }

            # è¾“å‡ºè¯¥Kå€¼çš„æµ‹è¯•ç»“æœ
            print(f"ğŸŒ K={K:2d} recall: {sum(recall_list) / test_count:.6f}")
            print(f"ğŸŒ K={K:2d} precision: {sum(precision_list) / test_count:.6f}")
            print(f"â±ï¸ K={K:2d} æ€»æµ‹è¯•æ—¶é—´: {k_total_time:.3f}ç§’\n")
            update_csv_with_results(csv_filepath, K, dataset_name, algorithm_type, total_results[percent_int], EXPERIMENT_GROUP)

        # ========== æœ€ç»ˆæ±‡æ€»æŠ¥å‘Š ==========
        print("=" * 80)
        print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•æ±‡æ€»æŠ¥å‘Š-{dataset_name}_{algorithm_type}_{K}%")
        print(f"ğŸš€ èŒƒå›´æŸ¥è¯¢æµ‹è¯•")
        print("=" * 80)

        # å®éªŒç»“æœ
        for percent_int in range(int_percent_first, int_percent_second + 1):
            result = total_results.get(percent_int)
            print(f"K={K:2d} range_para={percent_int}% çš„å®éªŒç»Ÿè®¡ç»“æœï¼š")
            print(f"â±ï¸ å¹³å‡æ˜æ–‡tokenç”Ÿæˆæ—¶é—´: {result['avg_token_time']:.8f}ç§’")
            print(f"â±ï¸ å¹³å‡hashåŠ å¯†tokenç”Ÿæˆæ—¶é—´: {result['avg_hash_token_time']:.8f}ç§’")
            print(f"â±ï¸ å¹³å‡åŒ¹é…æ—¶é—´: {result['avg_matched_time']:.8f}ç§’")
            print(f"â±ï¸ å¹³å‡å¯¹ç§°è§£å¯†æ—¶é—´: {result['avg_decrypt_time']:.8f}ç§’")
            print(f"â±ï¸ å¹³å‡ç²¾ç‚¼æ—¶é—´: {result['avg_refine_time']:.8f}ç§’")
            print(f"â±ï¸ å¹³å‡ç²¾ç¡®ç»“æœæ•°é‡: {result['avg_results_count']:.8f}ä¸ª")
            print(f"â±ï¸ å¹³å‡tokenæ•°é‡: {result['avg_token_count']:.8f}ä¸ª")
            print(f"â±ï¸ å¹³å‡å€™é€‰ç‚¹æ•°é‡: {result['avg_candidate_count']:.8f}ä¸ª")
            print(f"â±ï¸ å¹³å‡tokenå¤§å°: {result['avg_token_size']:.8f}ä¸ªå­—èŠ‚")
            print(f"â±ï¸ å¹³å‡å€™é€‰ç‚¹å¤§å°: {result['avg_candidate_size']:.8f}ä¸ªå­—èŠ‚")
            print(f"â±ï¸ å¹³å‡å¬å›ç‡: {result['avg_recall']:.8f}")
            print(f"â±ï¸ å¹³å‡ç²¾ç¡®ç‡: {result['avg_precision']:.8f}")
            print("-" * 60)

        print("-" * 80)
        # æ€»æ—¶é—´æ˜¯å½“å‰æ•°æ®é›†ä¸‹1%åˆ°5%èŒƒå›´æµ‹è¯•çš„æ—¶é—´ä¹‹å’Œ
        total_test_time = sum(result['test_time'] for result in total_results.values())
        print(f"â±ï¸ å…¨éƒ¨æµ‹è¯•æ€»æ—¶é—´: {total_test_time:.3f}ç§’")
        print("=" * 80)

