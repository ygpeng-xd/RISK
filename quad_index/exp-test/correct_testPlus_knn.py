import numpy as np
import pandas as pd
import time
import sys
import os

import InvertedIndex
import depth_test
import query_test
import AES_sha1

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from quad_index import knn
from quad_index import quad_token


def load_test_data(filepath):
    """åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼ˆæ”¯æŒå¤šå…³é”®è¯æ ¼å¼ï¼‰"""
    keywords_list = []  # å­˜å‚¨æ¯è¡Œçš„å…³é”®è¯åˆ—è¡¨
    points = []

    with open(filepath, 'r') as f:
        for line in f:
            line = line.strip()
            if not line:  # è·³è¿‡ç©ºè¡Œ
                continue

            # æ‰¾åˆ°æœ€åä¸€ä¸ªå·¦æ‹¬å·çš„ä½ç½®
            last_paren_index = line.rfind('(')
            if last_paren_index == -1:
                print(f"è­¦å‘Šï¼šæ— æ³•è§£æè¡Œ: {line}")
                continue

            # åˆ†ç¦»å…³é”®è¯éƒ¨åˆ†å’Œåæ ‡éƒ¨åˆ†
            keywords_part = line[:last_paren_index].strip()
            coord_part = line[last_paren_index:].strip()

            # è§£æå…³é”®è¯ï¼ˆç”¨ç©ºæ ¼åˆ†å‰²ï¼‰
            keywords = keywords_part.split()

            # è§£æåæ ‡ï¼ˆå»é™¤æ‹¬å·ï¼‰
            coord_str = coord_part.strip('()')
            x, y = coord_str.split()

            keywords_list.append(keywords)
            points.append([int(x), int(y)])

    return keywords_list, np.array(points)


def format_scientific(value, decimal_places=4):
    """
    å°†æ•°å€¼æ ¼å¼åŒ–ä¸ºç§‘å­¦è®¡æ•°æ³•ï¼Œä¿ç•™æŒ‡å®šå°æ•°ä½æ•°
    ä¾‹å¦‚ï¼š0.00002557 -> "2.5570e-05"
    è¿”å›floatç±»å‹ è¿™æ ·Excelèƒ½æ­£ç¡®è¯†åˆ«ä¸ºæ•°å€¼è€Œä¸æ˜¯æ–‡æœ¬
    """
    if value == 0:
        return "0.0000e+00"
    return float(f"{value:.{decimal_places}e}")


def update_csv_with_results(csv_filepath, K, dataset_name, algorithm_type, use_same_file_strategy, same_file_k, result,
                            group):
    """
    æ›´æ–°CSVæ–‡ä»¶ä¸­å¯¹åº”Kå€¼è¡Œçš„ç»Ÿè®¡ç»“æœ
    """
    # è¯»å–ç°æœ‰çš„CSVæ–‡ä»¶ï¼Œå¹¶æŒ‡å®šå­—ç¬¦ä¸²åˆ—çš„æ•°æ®ç±»å‹
    try:
        # æŒ‡å®šéœ€è¦å­˜å‚¨å­—ç¬¦ä¸²çš„åˆ—ä¸ºobjectç±»å‹
        dtype_dict = {
            'database': 'object',
        }
        df = pd.read_csv(csv_filepath, dtype=dtype_dict)
    except FileNotFoundError:
        print(f"âŒ CSVæ–‡ä»¶ä¸å­˜åœ¨: {csv_filepath}")
        return

    # æ‰¾åˆ°å¯¹åº”Kå€¼çš„è¡Œ
    k_row_index = df[(df['K'] == K) & (df['group'] == group)].index
    if len(k_row_index) == 0:
        # åˆ›å»ºæ–°è¡Œæ•°æ®
        print(f"ğŸ†• åˆ›å»ºæ–°è¡Œ: K={K}, group={group}")
        # è·å–æ–°è¡Œçš„ç´¢å¼•ï¼ˆä¸‹ä¸€ä¸ªå¯ç”¨çš„ç´¢å¼•ï¼‰
        k_row_index = len(df)
        # ä½¿ç”¨locç›´æ¥åˆ›å»ºæ–°è¡Œï¼Œé¿å…concatè­¦å‘Š
        df.loc[k_row_index, 'K'] = K
        df.loc[k_row_index, 'group'] = group
        # åˆå§‹åŒ–å…¶ä»–åˆ—ä¸ºé€‚å½“é»˜è®¤å€¼
        for col in df.columns:
            if col not in ['K', 'group']:
                df.loc[k_row_index, col] = None
    else:
        # å¦‚æœæ‰¾åˆ°äº†å¯¹åº”çš„è¡Œï¼Œè·å–ç¬¬ä¸€ä¸ªåŒ¹é…çš„è¡Œç´¢å¼•
        k_row_index = k_row_index[0]
        print(f"ğŸ”„ æ›´æ–°ç°æœ‰è¡Œ: K={K}, group={group}")

    # å¡«å†™åŸºæœ¬ä¿¡æ¯
    df.loc[k_row_index, 'database'] = dataset_name
    df.loc[k_row_index, 'KNN_query'] = 1  # KNNæŸ¥è¯¢å›ºå®šä¸º1
    df.loc[k_row_index, 'query_number'] = result['total_tests']
    df.loc[k_row_index, 'tree_min_depth'] = result['min_depth']
    df.loc[k_row_index, 'tree_max_depth'] = result['max_depth']

    # æ ¹æ®æ–‡ä»¶ä½¿ç”¨ç­–ç•¥å¡«å†™use_*_fileå­—æ®µ
    if use_same_file_strategy:
        # ä½¿ç”¨å„è‡ªçš„æ–‡ä»¶ç­–ç•¥
        if K == 1 and algorithm_type == 'voronoi':
            df.loc[k_row_index, 'use_voronoi_file'] = 1
        elif algorithm_type == 'center':
            df.loc[k_row_index, 'use_center_file'] = 1
        elif algorithm_type == 'centroid':
            df.loc[k_row_index, 'use_centroid_file'] = 1

    else:
        df.loc[k_row_index, 'use_same_file'] = 1
        # use_voronoi_fileä¸å¡«ï¼ˆå› ä¸ºä¸ä¼šä½¿ç”¨voronoiæ–‡ä»¶ï¼‰
        # æ³¨æ„ï¼šåœ¨åŒä¸€æ–‡ä»¶ç­–ç•¥ä¸‹ï¼Œalgorithm_typeä¸ä¼šæ˜¯'voronoi'
        # æ ¹æ®algorithm_typeå¡«å†™å¯¹åº”çš„æ–‡ä»¶ä½¿ç”¨æ ‡è®°
        if algorithm_type == 'center':
            df.loc[k_row_index, 'use_center_file'] = same_file_k
        elif algorithm_type == 'centroid':
            df.loc[k_row_index, 'use_centroid_file'] = same_file_k

    # å¡«å†™æ€§èƒ½ç»Ÿè®¡æ•°æ®ï¼ˆæ—¶é—´æ•°æ®ä½¿ç”¨ç§‘å­¦è®¡æ•°æ³•æ ¼å¼ï¼‰
    df.loc[k_row_index, 'token_time_avg(s)'] = format_scientific(result['avg_token_time'])
    df.loc[k_row_index, 'hash_token_time_avg(s)'] = format_scientific(result['avg_hash_token_time'])
    df.loc[k_row_index, 'matched_time_avg(s)'] = format_scientific(result['avg_matched_time'])
    df.loc[k_row_index, 'decrypt_time_avg(s)'] = format_scientific(result['avg_decrypt_time'])
    df.loc[k_row_index, 'refine_time_avg(s)'] = format_scientific(result['avg_refine_time'])

    # å…¶ä»–æ•°æ®ä¿æŒåŸæœ‰æ ¼å¼
    df.loc[k_row_index, 'results_count_avg'] = result['avg_results_count']
    df.loc[k_row_index, 'token_count_avg'] = result['avg_token_count']
    df.loc[k_row_index, 'candidate_count_avg'] = result['avg_candidate_count']
    df.loc[k_row_index, 'token_size_avg(bytes)'] = result['avg_token_size']
    df.loc[k_row_index, 'candidate_size_avg(bytes)'] = result['avg_candidate_size']
    df.loc[k_row_index, 'ratio_avg'] = round(result['ratio'], 4)
    # KNNæŸ¥è¯¢ä¸å¡«å†™recallå’Œprecisionï¼ˆä¿æŒä¸ºç©ºï¼‰

    # å¡«å†™æœ€åä¿®æ”¹æ—¶é—´
    df.loc[k_row_index, 'last_modify_time'] = int(time.strftime("%d%H%M%S", time.localtime()))

    # ä¿å­˜æ›´æ–°åçš„CSVæ–‡ä»¶
    df.to_csv(csv_filepath, index=False, encoding='utf-8', sep=',')
    print(f"âœ… å·²æ›´æ–°CSVæ–‡ä»¶ä¸­K={K}çš„ç»Ÿè®¡ç»“æœ")


if __name__ == '__main__':
    original_filepath = '../../original_dataset/Paris_RASK108.txt'
    test_filepath = '../../Adatabase/paris_test_2.txt'
    csv_filepath = '../../exp_result/data_statistics0828.csv'

    """
    é…ç½®åŒºåŸŸ - è‡ªåŠ¨æµ‹è¯•å¤šä¸ªKå€¼
    """
    K_values = [2, 4, 6, 8, 10]  # è¦æµ‹è¯•çš„Kå€¼åˆ—è¡¨
    dataset_name = 'paris'
    algorithm_type = 'center'  # 'center' æˆ–è€… 'centroid'
    base_dir = '../../Cdatabase'

    # å®éªŒç»„é…ç½®
    EXPERIMENT_GROUP = 3

    # æ§åˆ¶æ˜¯å¦ç”¨å„è‡ªçš„æ–‡ä»¶
    # use_own_file = True
    use_own_file = False
    same_file_k = 80

    x_min, x_max, y_min, y_max = 0, 100000000, 0, 100000000
    initial_bounds = [x_min, x_max, y_min, y_max]

    inverted_index = InvertedIndex.build_inverted_index_v2(original_filepath)
    test_keywords_list, test_points = load_test_data(test_filepath)

    # ========== æ±‡æ€»ç»“æœç»Ÿè®¡ ==========
    total_results = {}  # å­˜å‚¨æ¯ä¸ªKå€¼çš„æµ‹è¯•ç»“æœ

    # ========== å¾ªç¯æµ‹è¯•æ¯ä¸ªKå€¼ ==========
    for K in K_values:
        print("=" * 80)
        print(f"ğŸ” å¼€å§‹æµ‹è¯• K={K:2d}")
        print("=" * 80)

        if use_own_file:
            # ========== è‡ªåŠ¨ç”Ÿæˆè·¯å¾„ ==========
            if K == 1:
                algorithm_type = 'voronoi'
                not_voronoi_query = False
            else:
                algorithm_type = 'center'  # 'center' æˆ–è€… 'centroid'
                not_voronoi_query = True

            generate_filepath = f'{base_dir}/{dataset_name}_quadtree_{algorithm_type}{K}.txt'
            name, ext = os.path.splitext(generate_filepath)
            hash_filepath = f'{name}_hash{ext}'

        else:
            # ========== å…¨éƒ¨ç”¨K=10çš„æ–‡ä»¶ ==========
            generate_filepath = f'{base_dir}/{dataset_name}_quadtree_{algorithm_type}{same_file_k}.txt'
            hash_filepath = f'{base_dir}/{dataset_name}_quadtree_{algorithm_type}{same_file_k}_hash.txt'
            not_voronoi_query = True

        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(generate_filepath) or not os.path.exists(hash_filepath):
            print(f"âŒ K={K:2d} çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æµ‹è¯•")
            total_results[K] = "æ–‡ä»¶ä¸å­˜åœ¨"
            continue

        # ========== è‡ªåŠ¨è®¡ç®—æœ€å¤§æœ€å°æ·±åº¦ ==========
        max_depth, min_depth = [x // 2 for x in depth_test.count_max_min_length(generate_filepath)]

        hash_dict = {}
        with open(hash_filepath, 'r') as hash_file:
            for line in hash_file:
                line = line.strip()
                if line:  # è·³è¿‡ç©ºè¡Œ
                    parts = line.split(' ', 1)
                    if len(parts) >= 2:
                        hash_key = parts[0]  # c479d5c58198ab4141ea0a071290341566bcb1e6
                        hash_dict[hash_key] = line  # ä¿å­˜å®Œæ•´è¡Œ

        # è¿›è¡Œæ‰¹é‡æŸ¥è¯¢
        test_count = 0
        # ========== è¯¥Kå€¼æµ‹è¯•çš„å„ç§æ—¶é—´ç»Ÿè®¡ ==========
        k_time = []
        k_token_time = []  # ç»Ÿè®¡æ˜æ–‡tokenç”Ÿæˆæ—¶é—´
        k_hash_token_time = []  # ç»Ÿè®¡hash_tokenç”Ÿæˆæ—¶é—´
        k_matched_time = []  # ç»Ÿè®¡åŒ¹é…æ—¶é—´
        k_refine_time = []  # ç»Ÿè®¡ç²¾ç‚¼ç»“æœæ—¶é—´
        k_decrypt_time = []  # ç»Ÿè®¡å¯¹ç§°è§£å¯†æ—¶é—´
        # ========== ç»Ÿè®¡è¯¥Kå€¼æµ‹è¯•çš„å€™é€‰ç‚¹æ•°é‡ ==========
        result_counts = []  # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢ç²¾ç¡®ç»“æœçš„å¹³å‡ç‚¹æ•°é‡
        token_counts = []  # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢tokenæ•°é‡
        candidate_counts = []  # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢å€™é€‰é›†çš„å¹³å‡ç‚¹æ•°é‡
        token_sizes = []  # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢tokenå¤§å°--å­—èŠ‚æ•°
        candidate_sizes = []  # ç»Ÿè®¡å½“å‰Kå€¼100æ¬¡æŸ¥è¯¢å€™é€‰é›†å¤§å°--å­—èŠ‚æ•°
        # ========== ç²¾ç¡®ç‡ ==========
        ratio_list = []  # ç»Ÿè®¡ration

        for keywords, point in zip(test_keywords_list, test_points):
            test_count += 1
            keyword1 = keywords[0]
            keyword2 = keywords[1]
            print(f"[{test_count}/{len(test_keywords_list)}]--æŸ¥è¯¢ç‚¹: {point}")

            # è·å–ç¬¬ä¸€ä¸ªå…³é”®è¯å¯¹åº”çš„ç‚¹
            points1 = np.array(inverted_index[keyword1])
            # è·å–å®é™…ç‚¹æ•°å¹¶è°ƒæ•´Kå€¼
            available_points1 = len(np.unique(points1, axis=0))
            actual_k1 = min(K, available_points1)
            kd_tree1 = knn.build_kdtree(np.unique(points1, axis=0))
            neighbors1 = knn.find_k_nearest_neighbors(kd_tree1, point, actual_k1)
            nearest_points1 = [(int(neighbor[0]), int(neighbor[1])) for neighbor, _ in neighbors1]

            # è·å–ç¬¬äºŒä¸ªå…³é”®è¯å¯¹åº”çš„ç‚¹
            points2 = np.array(inverted_index[keyword2])
            available_points2 = len(np.unique(points2, axis=0))
            actual_k2 = min(K, available_points2)
            kd_tree2 = knn.build_kdtree(np.unique(points2, axis=0))
            neighbors2 = knn.find_k_nearest_neighbors(kd_tree2, point, actual_k2)
            nearest_points2 = [(int(neighbor[0]), int(neighbor[1])) for neighbor, _ in neighbors2]

            accurate_nearest_points = list(set(nearest_points1).intersection(set(nearest_points2)))

            query_point = tuple(point)

            # ========== å¼€å§‹è®¡æ—¶ ==========
            start_time1 = time.perf_counter()
            point_results = quad_token.query_quadtree(min_depth, max_depth, x_min, x_max, y_min, y_max, query_point)
            # ç»Ÿè®¡æ˜æ–‡tokenç”Ÿæˆæ—¶é—´
            token_time = time.perf_counter() - start_time1

            # ç¬¬ä¸€ä¸ªå…³é”®è¯è¿›è¡ŒåŒ¹é…
            start_time2 = time.perf_counter()
            # hash_token_timeç”¨äºç»Ÿè®¡hashåŠ å¯†tokenç”Ÿæˆæ—¶é—´
            print(f"  å…³é”®è¯1: {keyword1}")
            matched_data1, hash_token_time1, hash_token_list1 = query_test.match_query_results(keyword1, point_results,
                                                                                            hash_dict,
                                                                                            initial_bounds,
                                                                                            min_depth, max_depth,
                                                                                            not_voronoi_query)
            # ç»Ÿè®¡åŒ¹é…æ—¶é—´
            matched_time_with_hash_token1 = time.perf_counter() - start_time2
            matched_time1 = matched_time_with_hash_token1 - hash_token_time1

            # ç¬¬äºŒä¸ªå…³é”®è¯è¿›è¡ŒåŒ¹é…
            start_time2e = time.perf_counter()
            print(f"  å…³é”®è¯2: {keyword2}")
            matched_data2, hash_token_time2, hash_token_list2 = query_test.match_query_results(keyword2, point_results,
                                                                                               hash_dict,
                                                                                               initial_bounds,
                                                                                               min_depth, max_depth,
                                                                                               not_voronoi_query)
            matched_time_with_hash_token2 = time.perf_counter() - start_time2e
            matched_time2 = matched_time_with_hash_token2 - hash_token_time2

            # åˆå¹¶ä¸¤ä¸ªå…³é”®è¯çš„åŒ¹é…ç»“æœ
            matched_data = matched_data1 + matched_data2
            hash_token_list = hash_token_list1 + hash_token_list2
            hash_token_time = hash_token_time1 + hash_token_time2
            matched_time = matched_time1 + matched_time2

            start_time3 = time.perf_counter()
            result_set = set()
            for data in matched_data:
                parts = data.split()
                for part in parts[1:]:  # ä»ç¬¬äºŒä¸ªå…ƒç´ å¼€å§‹æ˜¯åæ ‡ç‚¹
                    result_set.add(part)

            result_tuples = [eval(item) for item in list(result_set)]

            # æŸ¥è¯¢æ—¶é•¿
            end_time = time.perf_counter()

            """
            ä¸‹é¢çš„è¿™ä¸€æ®µæ¨¡æ‹Ÿæš´æœç²¾ç‚¼ç»“æœ å®é™…ä¸Šå°±è”åˆä¸Šé¢çš„start_time3ç»Ÿè®¡ä¸ªæ—¶é•¿è€Œå·²
            """
            if len(result_tuples) == 0:
                refined_knn = []
            elif len(result_tuples) <= K:
                # å¦‚æœç»“æœé›†æ•°é‡ä¸è¶³Kä¸ªï¼Œå…¨éƒ¨è¿”å›å¹¶æŒ‰è·ç¦»æ’åº
                distances = [(pt, ((pt[0] - point[0]) ** 2 + (pt[1] - point[1]) ** 2) ** 0.5) for pt in result_tuples]
                distances.sort(key=lambda x: x[1])
                refined_knn = [pt for pt, dist in distances]
            else:
                # è®¡ç®—è·ç¦»å¹¶é€‰æ‹©æœ€è¿‘çš„Kä¸ªç‚¹
                distances = [(pt, ((pt[0] - point[0]) ** 2 + (pt[1] - point[1]) ** 2) ** 0.5) for pt in result_tuples]
                distances.sort(key=lambda x: x[1])
                refined_knn = [pt for pt, dist in distances[:K]]
            # ç»Ÿè®¡ç²¾ç‚¼æ—¶é—´
            refine_time = time.perf_counter() - start_time3

            """
            ä¸‹é¢çš„è¿™ä¸€æ®µæ¨¡å †æˆåŠ è§£å¯†è¿‡ç¨‹ï¼Œå®é™…ä¸Šå°±ç»Ÿè®¡ä¸ªè§£å¯†çš„æ—¶é•¿è€Œå·²
            """
            # ========== æ–°å¢ï¼šå¯¹ç§°è§£å¯†æ—¶é—´ç»Ÿè®¡ ==========
            decrypt_time = 0
            if AES_sha1 is not None and matched_data:
                # è®¾ç½®å¯¹ç§°åŠ å¯†å¯†é’¥
                symmetric_key = 'xducc02241931xdu'  # ä½¿ç”¨ä¸åŸä»£ç ç›¸åŒçš„å¯†é’¥

                # é¦–å…ˆæ¨¡æ‹ŸåŠ å¯†è¿‡ç¨‹ï¼šå°†æ¯æ¡æ•°æ®çš„ç‚¹åæ ‡éƒ¨åˆ†è¿›è¡Œå¯¹ç§°åŠ å¯†
                encrypted_data_list = []
                for data in matched_data:
                    parts = data.split(' ', 1)  # åˆ†å‰²hashç´¢å¼•å’Œç‚¹åæ ‡éƒ¨åˆ†
                    if len(parts) == 2:
                        hash_index = parts[0]
                        points_str = parts[1]  # æ‰€æœ‰ç‚¹åæ ‡çš„å­—ç¬¦ä¸²
                        # å¯¹ç‚¹åæ ‡éƒ¨åˆ†è¿›è¡ŒAESåŠ å¯†
                        encrypted_points = AES_sha1.AES_Encrypt(symmetric_key, points_str)
                        encrypted_data_list.append((hash_index, encrypted_points))

                # å¼€å§‹è®¡æ—¶ï¼šå¯¹ç§°è§£å¯†è¿‡ç¨‹
                start_time4 = time.perf_counter()
                # å¯¹æ¯æ¡åŠ å¯†æ•°æ®è¿›è¡Œè§£å¯†
                decrypted_data_list = []
                for hash_index, encrypted_points in encrypted_data_list:
                    # å¯¹å¯†æ–‡è¿›è¡ŒAESè§£å¯†
                    decrypted_points = AES_sha1.AES_Decrypt(symmetric_key, encrypted_points)
                    decrypted_data_list.append((hash_index, decrypted_points))
                # ç»“æŸè®¡æ—¶
                decrypt_time = time.perf_counter() - start_time4

            # ========== æ–°åŠ rationè®¡ç®—ç‰‡æ®µ ==========
            if len(accurate_nearest_points) == 0:
                k_ratio = 1.0
                ratio_list.append(k_ratio)
            else:
                # æ£€æŸ¥accurate_nearest_pointsæ˜¯å¦ä¸ºresult_tuplesçš„å­é›†
                if set(accurate_nearest_points).issubset(set(result_tuples)):
                    k_ratio = 1.0
                    ratio_list.append(k_ratio)
                else:
                    k_ratio = 0.0
                    ratio_list.append(k_ratio)

            # ========== ç»Ÿè®¡å„ç§å®éªŒç»“æœ ==========
            k_time.append(end_time - start_time1)
            k_token_time.append(token_time)
            k_hash_token_time.append(hash_token_time)
            k_matched_time.append(matched_time)
            k_refine_time.append(refine_time)
            k_decrypt_time.append(decrypt_time)
            result_counts.append(len(accurate_nearest_points))
            token_counts.append(len(hash_token_list))
            candidate_counts.append(len(result_tuples))
            token_sizes.append(sum(len(token.encode('utf-8')) for token in hash_token_list))
            candidate_sizes.append(sum(len(repr(tup).encode('utf-8')) for tup in result_tuples))

            print(f"  â±ï¸ æŸ¥è¯¢æ€»è€—æ—¶: {end_time - start_time1:.6f}ç§’")
            print(f"  â±ï¸ æ˜æ–‡tokenç”Ÿæˆè€—æ—¶: {token_time:.6f}ç§’")
            print(f"  â±ï¸ hashåŠ å¯†tokenç”Ÿæˆè€—æ—¶: {hash_token_time:.6f}ç§’")
            print(f"  â±ï¸ åŒ¹é…è€—æ—¶: {matched_time:.6f}ç§’")
            print(f"  â±ï¸ å¯¹ç§°è§£å¯†è€—æ—¶: {decrypt_time:.6f}ç§’")
            print(f"  â±ï¸ ç²¾ç‚¼è€—æ—¶: {refine_time:.6f}ç§’")
            print(f"  â±ï¸ ç²¾ç¡®è§£æ•°é‡: {len(accurate_nearest_points)}")
            print(f"  â±ï¸ tokenæ•°é‡: {len(hash_token_list)}")
            print(f"  â±ï¸ å€™é€‰ç‚¹æ•°é‡: {len(result_tuples)}")
            print(f"  â±ï¸ tokenå¤§å°: {sum(len(token.encode('utf-8')) for token in hash_token_list)}")
            print(f"  â±ï¸ å€™é€‰ç‚¹å¤§å°: {sum(len(repr(tup).encode('utf-8')) for tup in result_tuples)}")
            print(f"  â±ï¸ k_ratioè®¡ç®—ç»“æœ: {k_ratio:.6f}\n")

        # ========== è¯¥Kå€¼æµ‹è¯•ç»“æŸè®¡æ—¶ ==========
        k_total_time = sum(k_time)

        # è®°å½•è¯¥Kå€¼çš„æµ‹è¯•ç»“æœ
        total_results[K] = {
            'min_depth': min_depth,
            'max_depth': max_depth,
            'algorithm_type': algorithm_type,
            'total_tests': test_count,
            'test_time': k_total_time,
            'avg_token_time': sum(k_token_time) / test_count,
            'avg_hash_token_time': sum(k_hash_token_time) / test_count,
            'avg_matched_time': sum(k_matched_time) / test_count,
            'avg_refine_time': sum(k_refine_time) / test_count,
            'avg_decrypt_time': sum(k_decrypt_time) / test_count,
            'avg_results_count': sum(result_counts) / test_count,
            'avg_token_count': sum(token_counts) / test_count,
            'avg_candidate_count': sum(candidate_counts) / test_count,
            'avg_token_size': sum(token_sizes) / test_count,
            'avg_candidate_size': sum(candidate_sizes) / test_count,
            'ratio': sum(ratio_list) / len(ratio_list)  # è®¡ç®—è¯¥Kå€¼ä¸‹çš„rationè®¡ç®—ç»“æœ
        }

        # è¾“å‡ºè¯¥Kå€¼çš„æµ‹è¯•ç»“æœ
        print(f"ğŸŒ K={K:2d} ratio: {sum(ratio_list) / len(ratio_list):.6f}")
        print(f"â±ï¸ K={K:2d} æ€»æµ‹è¯•æ—¶é—´: {k_total_time:.3f}ç§’\n")

    # ========== æœ€ç»ˆæ±‡æ€»æŠ¥å‘Š ==========
    print("=" * 80)
    print(f"ğŸ“Š æœ€ç»ˆæµ‹è¯•æ±‡æ€»æŠ¥å‘Š-{algorithm_type}")
    print("=" * 80)

    # å®éªŒç»“æœ
    for K in K_values:
        result = total_results.get(K)
        print(f"K={K:2d}çš„å®éªŒç»Ÿè®¡ç»“æœï¼š")
        print(f"â±ï¸ å¹³å‡æ˜æ–‡tokenç”Ÿæˆæ—¶é—´: {result['avg_token_time']:.8f}ç§’")
        print(f"â±ï¸ å¹³å‡hashåŠ å¯†tokenç”Ÿæˆæ—¶é—´: {result['avg_hash_token_time']:.8f}ç§’")
        print(f"â±ï¸ å¹³å‡åŒ¹é…æ—¶é—´: {result['avg_matched_time']:.8f}ç§’")
        print(f"â±ï¸ å¹³å‡å¯¹ç§°è§£å¯†æ—¶é—´: {result['avg_decrypt_time']:.8f}ç§’")
        print(f"â±ï¸ å¹³å‡ç²¾ç‚¼æ—¶é—´: {result['avg_refine_time']:.8f}ç§’")
        print(f"â±ï¸ å¹³å‡ç²¾ç¡®è§£æ•°é‡: {result['avg_results_count']:.8f}ä¸ª")
        print(f"â±ï¸ å¹³å‡tokenæ•°é‡: {result['avg_token_count']:.8f}ä¸ª")
        print(f"â±ï¸ å¹³å‡å€™é€‰ç‚¹æ•°é‡: {result['avg_candidate_count']:.8f}ä¸ª")
        print(f"â±ï¸ å¹³å‡tokenå¤§å°: {result['avg_token_size']:.8f}ä¸ªå­—èŠ‚")
        print(f"â±ï¸ å¹³å‡å€™é€‰ç‚¹å¤§å°: {result['avg_candidate_size']:.8f}ä¸ªå­—èŠ‚")
        print(f"â±ï¸ ratioè®¡ç®—ç»“æœ: {result['ratio']:.6f}")
        # ========== æ–°å¢ï¼šæ›´æ–°CSVæ–‡ä»¶ ==========
        update_csv_with_results(csv_filepath, K, dataset_name, result['algorithm_type'], use_own_file, same_file_k, result, EXPERIMENT_GROUP)
        print("-" * 60)

    print("-" * 80)
    # æ€»æ—¶é—´æ˜¯æ‰€æœ‰Kå€¼æµ‹è¯•çš„æ—¶é—´ä¹‹å’Œ
    total_test_time = sum(result['test_time'] for result in total_results.values())
    print(f"â±ï¸ å…¨éƒ¨æµ‹è¯•æ€»æ—¶é—´: {total_test_time:.3f}ç§’")
    print("=" * 80)
